{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMZOyOAngx5Otzsdx/wNUBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinjaljoshi/llm_param_config/blob/main/prompt_bos_eos_tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D64pkI_AMolU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch bitsandbytes accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Define models\n",
        "MODELS = {\n",
        "    \"T5-Small\": {\"path\": \"t5-small\", \"type\": \"seq2seq\"},\n",
        "    \"Mistral\": {\"path\": \"mistralai/Mistral-7B-Instruct-v0.1\", \"type\": \"causal\"},\n",
        "    \"Qwen (8bit)\": {\"path\": \"Qwen/Qwen-7B-Chat\", \"type\": \"causal\", \"quantized\": True},\n",
        "    \"LLaMA\": {\"path\": \"meta-llama/Llama-2-7b-chat-hf\", \"type\": \"causal\"}\n",
        "}\n",
        "\n",
        "# Define prompt templates\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"without_bos_eos\": \"Summarize the following text: {text}\",\n",
        "    \"with_bos_eos\": \"<s>Summarize the following text: {text}</s>\"\n",
        "}\n",
        "\n",
        "# Test text input\n",
        "TEST_TEXT = \"Artificial intelligence is transforming various industries by automating tasks, improving efficiency, and enabling new capabilities.\"\n",
        "\n",
        "# Function to load model and tokenizer\n",
        "def load_model(model_info):\n",
        "    \"\"\"Loads the correct model type (Seq2Seq or Causal).\"\"\"\n",
        "    print(f\"\\nLoading {model_info['path']} (Quantized: {model_info.get('quantized', False)})...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_info[\"path\"])\n",
        "\n",
        "    if model_info[\"type\"] == \"seq2seq\":\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_info[\"path\"], torch_dtype=torch.float16, device_map=\"auto\")\n",
        "    else:  # Causal models (Mistral, Qwen, LLaMA)\n",
        "        if model_info.get(\"quantized\", False):\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_info[\"path\"],\n",
        "                torch_dtype=torch.float16,\n",
        "                load_in_8bit=True,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_info[\"path\"], torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Function to print BOS and EOS tokens\n",
        "def print_bos_eos_tokens(tokenizer, model_name):\n",
        "    \"\"\"Prints BOS and EOS tokens for the model.\"\"\"\n",
        "    bos_token = tokenizer.bos_token\n",
        "    eos_token = tokenizer.eos_token\n",
        "\n",
        "    print(f\"\\n{model_name} Token Details:\")\n",
        "    print(f\"  - BOS Token: {bos_token} (ID: {tokenizer.bos_token_id})\")\n",
        "    print(f\"  - EOS Token: {eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "\n",
        "    bos_eos_prompt = f\"{bos_token} Summarize the following text: {TEST_TEXT} {eos_token}\" if bos_token and eos_token else PROMPT_TEMPLATES[\"without_bos_eos\"]\n",
        "    return bos_eos_prompt\n",
        "\n",
        "# Function to run inference\n",
        "def run_inference(model_info, model_name):\n",
        "    \"\"\"Runs inference for a given model.\"\"\"\n",
        "    model, tokenizer = load_model(model_info)\n",
        "\n",
        "    # Print BOS and EOS tokens\n",
        "    bos_eos_prompt = print_bos_eos_tokens(tokenizer, model_name)\n",
        "\n",
        "    for template_type, template in PROMPT_TEMPLATES.items():\n",
        "        print(f\"\\nTesting {model_name} - {template_type} Prompt...\")\n",
        "\n",
        "        # Format prompt\n",
        "        prompt = bos_eos_prompt if template_type == \"with_bos_eos\" else template.format(text=TEST_TEXT)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        # Generate output\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**inputs, max_length=100)\n",
        "\n",
        "        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        print(f\"{model_name} Output ({template_type}):\\n{decoded_output}\")\n"
      ],
      "metadata": {
        "id": "B_qTH5bMNa9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load Flan-T5-Large\n",
        "MODEL_NAME = \"google/flan-t5-large\"\n",
        "\n",
        "print(f\"\\n Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Print EOS token details\n",
        "print(f\"\\nToken Details for {MODEL_NAME}:\")\n",
        "print(f\"  - EOS Token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "System: You are a helpful AI assistant that provides clear and concise answers.\n",
        "User: {user_input}\n",
        "Assistant:\n",
        "\"\"\"\n",
        "\n",
        "# Define test input\n",
        "TEST_TEXT = \"Summarize following text : Artificial intelligence is transforming industries by automating tasks, improving efficiency, and enabling new capabilities.\"\n",
        "\n",
        "formatted_prompt = PROMPT_TEMPLATE.format(user_input=TEST_TEXT)\n",
        "\n",
        "input_ids_without_eos = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Encode input WITH EOS\n",
        "input_ids_with_eos = tokenizer(formatted_prompt + \" </s>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generate output WITHOUT EOS\n",
        "with torch.no_grad():\n",
        "    output_without_eos = model.generate(input_ids_without_eos, max_length=100)\n",
        "decoded_output_without_eos = tokenizer.decode(output_without_eos[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate output WITH EOS\n",
        "with torch.no_grad():\n",
        "    output_with_eos = model.generate(input_ids_with_eos, max_length=100)\n",
        "decoded_output_with_eos = tokenizer.decode(output_with_eos[0], skip_special_tokens=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nOutput (Without EOS):\")\n",
        "print(decoded_output_without_eos)\n",
        "\n",
        "print(\"\\nOutput (With EOS):\")\n",
        "print(decoded_output_with_eos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcx6zNfpNrMb",
        "outputId": "4378c457-b280-4ae5-ce43-94750e699f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Loading google/flan-t5-large...\n",
            "\n",
            "Token Details for google/flan-t5-large:\n",
            "  - EOS Token: </s> (ID: 1)\n",
            "\n",
            "Output (Without EOS):\n",
            "You are a helpful AI assistant that provides clear and concise answers.\n",
            "\n",
            "Output (With EOS):\n",
            "Artificial intelligence is transforming industries by automating tasks, improving efficiency, and enabling new capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login, login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "login(hf_token)\n"
      ],
      "metadata": {
        "id": "FyI6hiWnSSjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load Mistral-7B-Instruct\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "print(f\"\\nLoading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Print BOS and EOS tokens\n",
        "print(f\"\\nToken Details for {MODEL_NAME}:\")\n",
        "print(f\"  - BOS Token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
        "print(f\"  - EOS Token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "\n",
        "# Define user input\n",
        "USER_INPUT = \"Summarize: Artificial intelligence is transforming industries by automating tasks, improving efficiency, and enabling new capabilities.\"\n",
        "\n",
        "# With [INST] Format\n",
        "PROMPT_WITH_INST = f\"\"\"\n",
        "<s> [INST] <<SYS>>\n",
        "You are a helpful AI assistant. Provide clear and concise responses.\n",
        "<</SYS>>\n",
        "\n",
        "{USER_INPUT} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# Without [INST] Format\n",
        "PROMPT_WITHOUT_INST = f\"\"\"\n",
        "<s> You are a helpful AI assistant. Provide clear and concise responses.\n",
        "\n",
        "{USER_INPUT}\n",
        "\"\"\"\n",
        "\n",
        "# Encode input\n",
        "input_ids_with_inst = tokenizer(PROMPT_WITH_INST, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "input_ids_without_inst = tokenizer(PROMPT_WITHOUT_INST, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generate output WITH [INST]\n",
        "with torch.no_grad():\n",
        "    output_with_inst = model.generate(input_ids_with_inst, max_length=100)\n",
        "decoded_output_with_inst = tokenizer.decode(output_with_inst[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate output WITHOUT [INST]\n",
        "with torch.no_grad():\n",
        "    output_without_inst = model.generate(input_ids_without_inst, max_length=100)\n",
        "decoded_output_without_inst = tokenizer.decode(output_without_inst[0], skip_special_tokens=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nOutput (With [INST] format):\")\n",
        "print(decoded_output_with_inst)\n",
        "\n",
        "print(\"\\n Output (Without [INST] format):\")\n",
        "print(decoded_output_without_inst)\n"
      ],
      "metadata": {
        "id": "d20dFCWZR6Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken transformers_stream_generator"
      ],
      "metadata": {
        "id": "DPFcuxMIUI9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load Qwen-7B-Chat\n",
        "MODEL_NAME = \"Qwen/Qwen-7B-Chat\"\n",
        "\n",
        "print(f\"\\nLoading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "# Print BOS and EOS tokens\n",
        "print(f\"\\n Token Details for {MODEL_NAME}:\")\n",
        "print(f\"  - BOS Token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
        "print(f\"  - EOS Token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "\n",
        "# Define user input\n",
        "USER_INPUT = \"Summarize: Artificial intelligence is transforming industries by automating tasks, improving efficiency, and enabling new capabilities.\"\n",
        "\n",
        "# With `<|im_start|>` Format\n",
        "PROMPT_WITH_INST = f\"\"\"\n",
        "<|im_start|>system\n",
        "You are a helpful AI assistant. Provide clear and concise responses.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{USER_INPUT}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "# Without `<|im_start|>` Format\n",
        "PROMPT_WITHOUT_INST = f\"\"\"\n",
        "You are a helpful AI assistant. Provide clear and concise responses.\n",
        "\n",
        "{USER_INPUT}\n",
        "\"\"\"\n",
        "\n",
        "# Encode input\n",
        "input_ids_with_inst = tokenizer(PROMPT_WITH_INST, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "input_ids_without_inst = tokenizer(PROMPT_WITHOUT_INST, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generate output WITH `<|im_start|>`\n",
        "with torch.no_grad():\n",
        "    output_with_inst = model.generate(input_ids_with_inst, max_length=100)\n",
        "decoded_output_with_inst = tokenizer.decode(output_with_inst[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate output WITHOUT `<|im_start|>`\n",
        "with torch.no_grad():\n",
        "    output_without_inst = model.generate(input_ids_without_inst, max_length=100)\n",
        "decoded_output_without_inst = tokenizer.decode(output_without_inst[0], skip_special_tokens=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nOutput (With `<|im_start|>` format):\")\n",
        "print(decoded_output_with_inst)\n",
        "\n",
        "print(\"\\nOutput (Without `<|im_start|>` format):\")\n",
        "print(decoded_output_without_inst)\n"
      ],
      "metadata": {
        "id": "TDCGEeu4S3q7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}