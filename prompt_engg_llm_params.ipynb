{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNVBK6GbsYXgGoLKiRRn65D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinjaljoshi/llm_param_config/blob/main/prompt_engg_llm_params.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAI : Summarize Dialogue\n",
        "\n",
        "We will compare one shot, and few shot inferences"
      ],
      "metadata": {
        "id": "7jfCGAKkCZA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarize Dialogue without Prompt Engineering\n",
        "\n",
        "* Data set DialogSum - https://huggingface.co/datasets/knkarthick/dialogsum"
      ],
      "metadata": {
        "id": "OK8nW019Ccn8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYjjc65yCP1Y"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
        "\n",
        "# Print a few samples\n",
        "for i in range(5):  # Print 5 samples from the dataset\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(dataset['train'][i])\n",
        "    print(\"-\" * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp3VUxBbELYK",
        "outputId": "3d4d29da-5392-47cf-e89f-26eebb55f2a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "{'id': 'train_0', 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\", 'topic': 'get a check-up'}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sample 2:\n",
            "{'id': 'train_1', 'dialogue': \"#Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\", 'summary': 'Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.', 'topic': 'vaccines'}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sample 3:\n",
            "{'id': 'train_2', 'dialogue': \"#Person1#: Excuse me, did you see a set of keys?\\n#Person2#: What kind of keys?\\n#Person1#: Five keys and a small foot ornament.\\n#Person2#: What a shame! I didn't see them.\\n#Person1#: Well, can you help me look for it? That's my first time here.\\n#Person2#: Sure. It's my pleasure. I'd like to help you look for the missing keys.\\n#Person1#: It's very kind of you.\\n#Person2#: It's not a big deal.Hey, I found them.\\n#Person1#: Oh, thank God! I don't know how to thank you, guys.\\n#Person2#: You're welcome.\", 'summary': \"#Person1#'s looking for a set of keys and asks for #Person2#'s help to find them.\", 'topic': 'find keys'}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sample 4:\n",
            "{'id': 'train_3', 'dialogue': \"#Person1#: Why didn't you tell me you had a girlfriend?\\n#Person2#: Sorry, I thought you knew.\\n#Person1#: But you should tell me you were in love with her.\\n#Person2#: Didn't I?\\n#Person1#: You know you didn't.\\n#Person2#: Well, I am telling you now.\\n#Person1#: Yes, but you might have told me before.\\n#Person2#: I didn't think you would be interested.\\n#Person1#: You can't be serious. How dare you not tell me you are going to marry her?\\n#Person2#: Sorry, I didn't think it mattered.\\n#Person1#: Oh, you men! You are all the same.\", 'summary': \"#Person1#'s angry because #Person2# didn't tell #Person1# that #Person2# had a girlfriend and would marry her.\", 'topic': 'have a girlfriend'}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sample 5:\n",
            "{'id': 'train_4', 'dialogue': \"#Person1#: Watsup, ladies! Y'll looking'fine tonight. May I have this dance?\\n#Person2#: He's cute! He looks like Tiger Woods! But, I can't dance. . .\\n#Person1#: It's all good. I'll show you all the right moves. My name's Malik.\\n#Person2#: Nice to meet you. I'm Wen, and this is Nikki.\\n#Person1#: How you feeling', vista? Mind if I take your friend'round the dance floor?\\n#Person2#: She doesn't mind if you don't mind getting your feet stepped on.\\n#Person1#: Right. Cool! Let's go!\", 'summary': \"Malik invites Nikki to dance. Nikki agrees if Malik doesn't mind getting his feet stepped on.\", 'topic': 'dance'}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF6JtJOHEXoQ",
        "outputId": "bbe648d7-5f66-4a1f-ffb3-04be27af51fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (12460, 4), 'validation': (500, 4), 'test': (1500, 4)}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the FLAN-T5 model, creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method.\n",
        "\n",
        "To perform encoding and decoding, you need to work with text in a tokenized form. **Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models.\n",
        "\n",
        "Download the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer.\n",
        "\n",
        "* The use_fast=True argument ensures that a fast tokenizer (based on the Rust tokenizers library) is used, which is much more efficient than the standard Python-based tokenizer.\n",
        "* Fast tokenizers are optimized for speed and provide better handling of special cases like wordpiece or byte-pair encoding (BPE)."
      ],
      "metadata": {
        "id": "0tX04H43EcRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "#Load pretrained model and tokenizer\n",
        "model_name='google/flan-t5-base'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "#AutoTokenizer will load internally tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "f8Ck96gnEfE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate output from base model without any Prompt Engineering"
      ],
      "metadata": {
        "id": "EkfTiymSErOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"What is the evaporation ?\"\n",
        "\n",
        "tokenized_input = tokenizer(sample_sentence, return_tensors='pt').to('cuda')\n",
        "\n",
        "model.to('cuda')\n",
        "tokenized_input = tokenized_input.to('cuda')\n",
        "\n",
        "output = model.generate(\n",
        "        input_ids=tokenized_input[\"input_ids\"],\n",
        "        max_new_tokens=100,\n",
        "    )\n",
        "decoded_output = tokenizer.decode(\n",
        "        output[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print(decoded_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAsq_J5GEvX9",
        "outputId": "dcdb5596-e065-4e03-bb77-a2e6361cb357"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaporation of water\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample dialogue for summary"
      ],
      "metadata": {
        "id": "5RYI1GgSE1rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_indexes = [10,20,30]\n",
        "for i, index in enumerate(sample_indexes):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    inputs = tokenizer(dialogue, return_tensors='pt').to('cuda')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=100,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print('-' *100)\n",
        "    print('Sample ', i + 1)\n",
        "    print('-' *100)\n",
        "    print(f'Dialogue:\\n{dialogue}')\n",
        "    print('-' *100)\n",
        "    print(f'Labelled Summary:\\n{summary}')\n",
        "    print('-' *100)\n",
        "    print(f'Model Output:\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhUPbVOME803",
        "outputId": "18b53e06-bd0b-4ded-8b50-717c665507b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Sample  1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Happy Birthday, this is for you, Brian.\n",
            "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
            "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
            "#Person2#: Ok.\n",
            "#Person1#: This is really wonderful party.\n",
            "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
            "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
            "#Person2#: You look great, you are absolutely glowing.\n",
            "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output:\n",
            "Brian, thank you for coming to our party.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sample  2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: What's wrong with you? Why are you scratching so much?\n",
            "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
            "#Person1#: Let me have a look. Whoa! Get away from me!\n",
            "#Person2#: What's wrong?\n",
            "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
            "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
            "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
            "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output:\n",
            "Person1#: I'm scratching so much. I can't stand it anymore. I think I may be coming down with something. I feel lightheaded and weak.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sample  3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Where are you going for your trip?\n",
            "#Person2#: I think Hebei is a good place.\n",
            "#Person1#: But I heard the north of China are experiencing severe sandstorms!\n",
            "#Person2#: Really?\n",
            "#Person1#: Yes, it's said that Hebes was experiencing six degree strong winds.\n",
            "#Person2#: How do these storms affect the people who live in these areas?\n",
            "#Person1#: The report said the number of people with respiratory tract infections tended to rise after sandstorms. The sand gets into people's noses and throats and creates irritation.\n",
            "#Person2#: It sounds that sandstorms are trouble for everybody!\n",
            "#Person1#: You are quite right.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person2# plans to have a trip in Hebei but #Person1# says there are sandstorms in there.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output:\n",
            "People in Hebei are experiencing severe sandstorms.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero shot Vs Few Shot Inference"
      ],
      "metadata": {
        "id": "als0HBhwFBR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Zero-Shot Inference refers to the ability of a machine learning model (especially a large language model) to perform a task without having been explicitly trained on that specific task. Instead, the model generalizes its learned knowledge from related tasks to make predictions in a completely new scenario.\n"
      ],
      "metadata": {
        "id": "WKZD0jIXFLaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(sample_indexes):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Create a summary of the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the dialogue.\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "\n",
        "    print('-' *100)\n",
        "    print('Sample ', i + 1)\n",
        "    print('-' *100)\n",
        "    print(f'Dialogue:\\n{dialogue}')\n",
        "    print('-' *100)\n",
        "    print(f'Labelled Summary:\\n{summary}')\n",
        "    print('-' *100)\n",
        "    print(f'Model Output - Zero Shot :\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOiF1vSIFJk9",
        "outputId": "2a361fbb-ec21-49c2-9fd3-de1fe82fdc7e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Sample  1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Happy Birthday, this is for you, Brian.\n",
            "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
            "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
            "#Person2#: Ok.\n",
            "#Person1#: This is really wonderful party.\n",
            "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
            "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
            "#Person2#: You look great, you are absolutely glowing.\n",
            "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Zero Shot :\n",
            "Brian's birthday is coming up.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sample  2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: What's wrong with you? Why are you scratching so much?\n",
            "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
            "#Person1#: Let me have a look. Whoa! Get away from me!\n",
            "#Person2#: What's wrong?\n",
            "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
            "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
            "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
            "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Zero Shot :\n",
            "Person1 is scratching a lot.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sample  3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Where are you going for your trip?\n",
            "#Person2#: I think Hebei is a good place.\n",
            "#Person1#: But I heard the north of China are experiencing severe sandstorms!\n",
            "#Person2#: Really?\n",
            "#Person1#: Yes, it's said that Hebes was experiencing six degree strong winds.\n",
            "#Person2#: How do these storms affect the people who live in these areas?\n",
            "#Person1#: The report said the number of people with respiratory tract infections tended to rise after sandstorms. The sand gets into people's noses and throats and creates irritation.\n",
            "#Person2#: It sounds that sandstorms are trouble for everybody!\n",
            "#Person1#: You are quite right.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person2# plans to have a trip in Hebei but #Person1# says there are sandstorms in there.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Zero Shot :\n",
            "The sandstorms in China are causing a lot of health problems for people in the north of China.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLAN-T5 prompt templates that are published  [here](https://github.com/google-research/FLAN/tree/main/flan/v2).\n",
        "\n",
        "\n",
        "A **stop sequence** is a specific string of characters or tokens that signals an LLM to stop generating further output. It helps in controlling the output length and ensuring structured responses.\n",
        "\n",
        "The stop sequence '\\n\\n\\n' is used for FLAN-T5\n",
        "\n"
      ],
      "metadata": {
        "id": "-lbg6M6kFYBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####One Shot Inference\n",
        "The model is given only one example (a single demonstration) to understand and perform a new task.\n"
      ],
      "metadata": {
        "id": "VOVHtR_XFcFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_prompt(example_indices, test_indices):\n",
        "  #Give an example\n",
        "    prompt = ''\n",
        "    for index in example_indices:\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "        prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "{summary}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    #Test dialogue\n",
        "    dialogue = dataset['test'][test_indices]['dialogue']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "one_shot_prompt = make_inference_prompt([30], 20)\n",
        "print(one_shot_prompt)\n",
        "\n",
        "\n",
        "#Use above prompt to process summary\n",
        "label_summary = dataset['test'][20]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt').to('cuda')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Labelled Summary:\\n{label_summary}')\n",
        "print('-' *100)\n",
        "print(f'Model Output - One Shot :\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM7z4EjrFi9N",
        "outputId": "8385bed2-a6c3-45eb-f4ff-1a92ace98d86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Where are you going for your trip?\n",
            "#Person2#: I think Hebei is a good place.\n",
            "#Person1#: But I heard the north of China are experiencing severe sandstorms!\n",
            "#Person2#: Really?\n",
            "#Person1#: Yes, it's said that Hebes was experiencing six degree strong winds.\n",
            "#Person2#: How do these storms affect the people who live in these areas?\n",
            "#Person1#: The report said the number of people with respiratory tract infections tended to rise after sandstorms. The sand gets into people's noses and throats and creates irritation.\n",
            "#Person2#: It sounds that sandstorms are trouble for everybody!\n",
            "#Person1#: You are quite right.\n",
            "\n",
            "Summary:\n",
            "#Person2# plans to have a trip in Hebei but #Person1# says there are sandstorms in there.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What's wrong with you? Why are you scratching so much?\n",
            "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
            "#Person1#: Let me have a look. Whoa! Get away from me!\n",
            "#Person2#: What's wrong?\n",
            "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
            "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
            "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
            "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
            "\n",
            "Summary:\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - One Shot :\n",
            "Person1 is scratching so much that he can't stand it anymore. He feels lightheaded and weak.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Few Shot Inference\n",
        "\n",
        "The model is given a few examples (typically 5 to 10) before performing the task.The model generalizes based on multiple demonstrations and applies the learned pattern to new inputs."
      ],
      "metadata": {
        "id": "aF3DrRh0F0xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = make_inference_prompt([1,5,10,15,25,30,35], 20)\n",
        "print(few_shot_prompt)\n",
        "\n",
        "\n",
        "#Use above prompt to process summary\n",
        "label_summary = dataset['test'][20]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to('cuda')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Labelled Summary:\\n{label_summary}')\n",
        "print('-' *100)\n",
        "print(f'Model Output - One Shot :\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxdfLlpTF9y4",
        "outputId": "0fe7c1dc-8901-4307-afc4-b4924200455b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2402 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "\n",
            "Summary:\n",
            "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: You're finally here! What took so long?\n",
            "#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n",
            "#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n",
            "#Person2#: I don't think it can be avoided, to be honest.\n",
            "#Person1#: perhaps it would be better if you started taking public transport system to work.\n",
            "#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n",
            "#Person1#: It would be better for the environment, too.\n",
            "#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n",
            "#Person1#: Taking the subway would be a lot less stressful than driving as well.\n",
            "#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n",
            "#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n",
            "#Person2#: That's true. I could certainly use the exercise!\n",
            "#Person1#: So, are you going to quit driving to work then?\n",
            "#Person2#: Yes, it's not good for me or for the environment.\n",
            "\n",
            "Summary:\n",
            "#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Happy Birthday, this is for you, Brian.\n",
            "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
            "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
            "#Person2#: Ok.\n",
            "#Person1#: This is really wonderful party.\n",
            "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
            "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
            "#Person2#: You look great, you are absolutely glowing.\n",
            "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
            "\n",
            "Summary:\n",
            "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: I've had it! I am done working for a company that is taking me nowhere!\n",
            "#Person2#: So what are you gonna do? Just quit?\n",
            "#Person1#: That's exactly what I am going to do! I have decided to create my own company! I am going to write up a business plan, get some investors and start working for myself!\n",
            "#Person2#: Have you ever written up a business plan before?\n",
            "#Person1#: Well, no, it can't be that hard! I mean, all you have to do is explain your business, how you are going to do things and that's it, right?\n",
            "#Person2#: You couldn't be more wrong! A well written business plan will include an executive summary which highlights the idea of the business in two pages or less. Then you need to describe your company with information such as what type of legal structure it has, history, etc.\n",
            "#Person1#: Well, that seems easy enough.\n",
            "#Person2#: Wait, there is more! Then you need to introduce and describe your goods or services. What they are and how they are different from competitors? Then comes the hard part, a market analysis. You need to investigate and analyze hundreds of variables! You need to take into consideration socioeconomic factors from GDP per capita to how many children on average the population has! All this information is useful so that you can move on to your strategy and implementation stage, where you will describe in detail how you will actually execute your idea.\n",
            "#Person1#: Geez. Is that all?\n",
            "#Person2#: Almost, the most important piece of information for your investors will be the financial analysis. Here you will calculate and estimate sales, cash flow and profits. After all, people will want to know when they will begin to see a return on their investment!\n",
            "#Person1#: Umm. I think I ' ll just stick to my old job and save myself all the hassle of trying to start up a business!\n",
            "\n",
            "Summary:\n",
            "#Person1# wants to create a company and is going to write a business plan. #Person2# gives #Person1# suggestions on how to summarise business ideas, describe the service, differ from competitors and attract investment in a good business plan. #Person1# decides to stick to the old job.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Steven, I need badly your help.\n",
            "#Person2#: What's the matter?\n",
            "#Person1#: My wife has found that I have an affair with my secretary, and now she is going to divorce me.\n",
            "#Person2#: How could you cheat on your wife? You have been married for ten years.\n",
            "#Person1#: Yes, I know I'm wrong. But I swear that the affair lasts only for two months. And I still love my wife. I couldn't live without her.\n",
            "#Person2#: I will try my best to persuade her to reconsider the divorce. But are you sure that from now on you will be faithful to her forever?\n",
            "#Person1#: Yes, I swear.\n",
            "\n",
            "Summary:\n",
            "Steve will try to persuade #Person1#'s wife not to divorce #Person1# as #Person1# swears to remain faithful forever.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Where are you going for your trip?\n",
            "#Person2#: I think Hebei is a good place.\n",
            "#Person1#: But I heard the north of China are experiencing severe sandstorms!\n",
            "#Person2#: Really?\n",
            "#Person1#: Yes, it's said that Hebes was experiencing six degree strong winds.\n",
            "#Person2#: How do these storms affect the people who live in these areas?\n",
            "#Person1#: The report said the number of people with respiratory tract infections tended to rise after sandstorms. The sand gets into people's noses and throats and creates irritation.\n",
            "#Person2#: It sounds that sandstorms are trouble for everybody!\n",
            "#Person1#: You are quite right.\n",
            "\n",
            "Summary:\n",
            "#Person2# plans to have a trip in Hebei but #Person1# says there are sandstorms in there.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Welcome to my birthday party, I am so happy you can come.\n",
            "#Person2#: Thanks for inviting me. Here is the gift for you. Happy birthday, Francis! Many more happy and healthy years for you!\n",
            "#Person1#: Thank you, shall I open it now?\n",
            "#Person2#: Yes, please do.\n",
            "#Person1#: Wow, a remote car model and my favorite brand. I really like it. That is so nice of you.\n",
            "#Person2#: Yeah, I was really struggling whether I should give you this nice little car. It was the last one they had and I really like it so much myself.\n",
            "#Person1#: Typical you, always wanting to keep the best things for yourself. The more I appreciate the gift now.\n",
            "\n",
            "Summary:\n",
            "#Person2# gives Francis a nice car model as a birthday gift and Francis appreciates it.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What's wrong with you? Why are you scratching so much?\n",
            "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
            "#Person1#: Let me have a look. Whoa! Get away from me!\n",
            "#Person2#: What's wrong?\n",
            "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
            "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
            "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
            "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
            "\n",
            "Summary:\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - One Shot :\n",
            "Person1 is scratching so much that he can't stand it anymore. He feels lightheaded and weak.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can modify the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the [here](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig).\n",
        "\n",
        "\n",
        "Some Important ones are as under\n",
        "**Parameters that control the generation strategy used**\n",
        "\n",
        "1. **do_sample** (bool, optional, defaults to False) — Whether or not to use sampling ; use greedy decoding otherwise.\n",
        "2. **num_beams** (int, optional, defaults to 1) — Number of beams for beam search. 1 means no beam search.\n",
        "3. **num_beam_groups** (int, optional, defaults to 1) — Number of groups to divide num_beams into in order to ensure diversity among different groups of beams. this paper for more details.\n",
        "4. **use_cache** (bool, optional, defaults to True) — Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
        "\n",
        "**Parameters for manipulation of the model output logits**\n",
        "\n",
        "1. **temperature** (float, optional, defaults to 1.0) — The value used to modulate the next token probabilities.\n",
        "2. **top_k** (int, optional, defaults to 50) — The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "3. **top_p** (float, optional, defaults to 1.0) — If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation."
      ],
      "metadata": {
        "id": "hDNTJWqjGMea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temperature - Controls the randomness of token selection.\n",
        "\n",
        "* Higher values (>1.0) → More randomness (creative, diverse outputs).\n",
        "* Lower values (< 1.0) → More deterministic (focused, repetitive).\n",
        "* At 0, always picks the highest-probability token)."
      ],
      "metadata": {
        "id": "GC_QVbpWHdYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = make_inference_prompt([1,5,10,15,25,30,35], 100)\n",
        "#print(few_shot_prompt)\n",
        "\n",
        "\n",
        "#Use above prompt to process summary\n",
        "label_summary = dataset['test'][100]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to('cuda')\n",
        "output_temp_0_1 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        temperature=0.1\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_temp_0_7 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        temperature=0.7\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_temp_1 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        temperature=1.0\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_temp_1_5 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        temperature=1.5\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Labelled Summary:\\n{label_summary}')\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot temperature 0.1 :\\n{output_temp_0_1}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot temperature 0.7:\\n{output_temp_0_7}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot temperature 1.0:\\n{output_temp_1}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot temperature 1.5:\\n{output_temp_1_5}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5oy_rigH3gU",
        "outputId": "29ee2fc5-04a2-4f90-9c91-424bbdd9d2d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot temperature 0.1 :\n",
            "The two men are trying to figure out how to react to a cut.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot temperature 0.7:\n",
            "#Person1#: Jason and Laura have been together for three years.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot temperature 1.0:\n",
            "A close up of their relationship. They agree on how they make their lines.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot temperature 1.5:\n",
            "While talking you can probably say an even negative comment about Jason's relationship and Jason's sadness and anger.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***top_k sampling*** Limits the model to selecting from only the top k most probable tokens at each step."
      ],
      "metadata": {
        "id": "DgM3zzIWJl-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = make_inference_prompt([1,5,10,15,25,30,35], 100)\n",
        "#print(few_shot_prompt)\n",
        "\n",
        "\n",
        "#Use above prompt to process summary\n",
        "label_summary = dataset['test'][100]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to('cuda')\n",
        "output_top10 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        top_k=10\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_top20 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        top_k=20\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_top30 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        top_k=30\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_top40 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        top_k=40\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Labelled Summary:\\n{label_summary}')\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot top_k - 10 :\\n{output_top10}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot top_k - 20:\\n{output_top20}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot top_k - 30:\\n{output_top30}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot top_k - 40:\\n{output_top40}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_aos483Jt4i",
        "outputId": "9c9ea232-4456-4ba1-8da0-f0b67eed5e98"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot top_k - 10 :\n",
            "#Person1: Jason and Laura will try to make it work for them by talking to each other.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot top_k - 20:\n",
            "Jason wants to tell someone his feelings but Mike is frustrated with her.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot top_k - 30:\n",
            "The two guys talked about the problem of Mike and Laura with others.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot top_k - 40:\n",
            "They discuss how this time there was some anger in some of the men's conversation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***top_p*** - Instead of a fixed number (k), top_p dynamically selects tokens whose cumulative probability adds up to top_p."
      ],
      "metadata": {
        "id": "mQCWorkUKjx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = make_inference_prompt([1,5,10,15,25,30,35], 100)\n",
        "#print(few_shot_prompt)\n",
        "\n",
        "\n",
        "#Use above prompt to process summary\n",
        "label_summary = dataset['test'][100]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt').to('cuda')\n",
        "output_top0_5 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        top_p=0.5\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_top0_7 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        top_p=0.7\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_top0_8 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        top_p=0.8\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "output_top1 = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample = True,\n",
        "        top_p=1.0\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Labelled Summary:\\n{label_summary}')\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot top_p=0.5  :\\n{output_top0_5}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot top_p=0.7 :\\n{output_top0_7}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot top_p=0.8 :\\n{output_top0_8}\\n')\n",
        "\n",
        "print('-' *100)\n",
        "print(f'Model Output - Few Shot top_p=1.0 :\\n{output_top1}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmHeyv-LKty3",
        "outputId": "595902c0-807f-4e26-ee83-91cc2c9f0274"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Labelled Summary:\n",
            "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot top_p=0.5  :\n",
            "Mike and Laura will try to find a solution to the problem that Jason and Laura have been together for three years.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot top_p=0.7 :\n",
            "The problem with Jason and Laura is that they are not in a happy or positive mood.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot top_p=0.8 :\n",
            "A cut may be a solution, but Mike and Laura aren't convinced.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Output - Few Shot top_p=1.0 :\n",
            "Those of you who think that Jason's reaction would be a combination of anger and sadness, will probably try to change the story.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}